---
title: "class08"
author: "Nate Tran"
format: pdf
---

# Data Preparation

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
wisc.data <- wisc.df[,-1]
diagnosis <- as.factor(wisc.df$diagnosis)
```

## Q1
There are 569 observations in the dataset.
```{r}
nrow(wisc.df)
```

## Q2
There are 212 observations with malignant diagnoses.
```{r}
table(diagnosis)
```

## Q3
There are 10 features in the data suffixed with "_mean".
```{r}
length(grep("_mean",colnames(wisc.df)))
```

```{r}
#checking if wisc.data needs to be scaled
round(colMeans(wisc.data), 2)
round(apply(wisc.data, 2, sd), 2)
```

The data needs to be scaled, since the mean and SD values are so variant.

#PCA Time

```{r}
wisc.pr <- prcomp(wisc.data, scale=T)
summary(wisc.pr)
```

## Q4
44.27% of the original variance is captured by PC1.

## Q5
3 PCs are required to describe at least 70% of the original variance.

## Q6
7 PCs are required to describe at least 90% of the original variance.

# Interpreting PCA

Making scree plot is good way to find "intrinsic dimensionality" of the dataset

```{r}
attributes(wisc.pr)
#calculate variance by squaring SD
var <- wisc.pr$sdev^2

#calculate proportion of variance due to each PC
pve <- var/sum(var)

plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


```{r}
biplot(wisc.pr)
```

## Q7
Nothing stands out about this plot aside from its excessive messiness and extreme difficulty in interpreting. This is primarily due to the large amount of observations and variables we are trying to observe here.

## Q8
These plots have less and less clear separation between malignant and benign samples when using less important PCs.

```{r}
plot(wisc.pr$x[,c(1,3)], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```


Plotting PC plot with ggplot!
```{r}
library(ggplot2)

pc <- as.data.frame(wisc.pr$x)

ggplot(pc) +
  aes(PC1, PC2, col=diagnosis) +
  geom_point()
```

## Q9
The feature concave.points_mean contributes -0.261 to the first PC.
```{r}
wisc.pr$rotation[,1]
```

# Hierarchical clustering

## Preparing Data

```{r}
data_scaled <- scale(wisc.data)
data_dist <- dist(data_scaled)
wisc.hclust <- hclust(data_dist, method="complete")
```

## Q10
With a height of 19, the model using a complete clustering method has 4 clusters.
```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

Cutting tree into 4 clusters and assigning membership
```{r}
wisc.clusters <- cutree(wisc.hclust, k=4)
table(wisc.clusters, diagnosis)
```

## Q12
The ward.D2 method gives the best looking results with two more distinct groupings based on the dendrogram, likely correlating with malignant and benign diagnoses.

# Combining Methods: PCA and hclust
We can cluster in PC-space using as many or few PCs as we want!

```{r}
#clustering on 3 PC-space
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:3]), method="ward.D2")

plot(wisc.pr.hclust)
```

Finding out whether these two main clusters correlate with malignant and benign

```{r}
groups <- cutree(wisc.pr.hclust, k=2)
table(groups, diagnosis)
```
Calculating accuracy of our clustering method with verified diagnoses

```{r}
(179+333)/nrow(wisc.data)
```


## Q13
The newly created model separates out the two different diagnoses very well with an accuracy of about 90%.

## Q14
The other hierarchical clustering models created in previous sections perform similarly to the combined PCA/clustering approach
```{r}
table(wisc.clusters, diagnosis)
(165+343)/nrow(wisc.data)
```

#Prediction!

```{r}
new_data <- read.csv("https://tinyurl.com/new-samples-CSV")
new_predict <- predict(wisc.pr, newdata=new_data)
new_predict
```

Plotting new_data predictions against the wisc.pr PC plot
```{r}
plot(wisc.pr$x[,1:2], col= diagnosis)
points(new_predict[,1], new_predict[,2], col="blue", pch=16, cex=3)
text(new_predict[,1], new_predict[,2], c(1,2), col="white")
```


## Q16
We should prioritize patient #2 for followup based on the prediction results since patient #2 clusters with other patients with malignant diagnoses.